{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b53607a-4906-4475-a2ef-3d6bf5a68f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 14:41:51.587282: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 14:41:51.716453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740820311.780608    7233 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740820311.800140    7233 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-01 14:41:51.915997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e2281f-74b0-44be-9132-d9ff8f5c2d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "I0000 00:00:1740820320.634014    7233 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5315 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e327194-1b09-4b73-a453-c138612a8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_right(input_ids, pad_token_id, decoder_start_token_id):\n",
    "    \"\"\"\n",
    "    Shifts input ids to the right by one position.\n",
    "    The first token becomes the decoder_start_token_id.\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(input_ids)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], decoder_start_token_id)\n",
    "    shifted_input_ids = tf.concat([start_tokens, input_ids[:, :-1]], axis=1)\n",
    "    return shifted_input_ids\n",
    "\n",
    "class MultiTaskT5(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_emotion_labels, num_sentiment_labels):\n",
    "        super(MultiTaskT5, self).__init__()\n",
    "        self.t5 = TFT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.emotion_classifier = tf.keras.layers.Dense(\n",
    "            num_emotion_labels, activation='softmax', name=\"emotion_classifier\"\n",
    "        )\n",
    "        self.sentiment_classifier = tf.keras.layers.Dense(\n",
    "            num_sentiment_labels, activation='softmax', name=\"sentiment_classifier\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, task, training=False, labels=None):\n",
    "        if task in ['emotion', 'sentiment']:\n",
    "            encoder_outputs = self.t5.encoder(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                training=training\n",
    "            )\n",
    "            pooled_output = tf.reduce_mean(encoder_outputs.last_hidden_state, axis=1)\n",
    "            pooled_output = self.dropout(pooled_output, training=training)\n",
    "            if task == 'emotion':\n",
    "                return self.emotion_classifier(pooled_output)\n",
    "            else:\n",
    "                return self.sentiment_classifier(pooled_output)\n",
    "        elif task == 'summary':\n",
    "            if labels is None:\n",
    "                outputs = self.t5(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    training=training\n",
    "                )\n",
    "                return outputs.logits\n",
    "            else:\n",
    "                pad_token_id = self.t5.config.pad_token_id\n",
    "                decoder_start_token_id = self.t5.config.decoder_start_token_id\n",
    "                decoder_input_ids = shift_right(labels, pad_token_id, decoder_start_token_id)\n",
    "                outputs = self.t5(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    decoder_input_ids=decoder_input_ids,\n",
    "                    training=training\n",
    "                )\n",
    "                return outputs.logits\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported task type. Use 'emotion', 'sentiment', or 'summary'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8729ca29-20cc-41e2-a89e-cc40819f82d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "num_emotion_labels = 6\n",
    "num_sentiment_labels = 4  # Updated to 4 output labels for sentiment\n",
    "model = MultiTaskT5(model_name, num_emotion_labels, num_sentiment_labels)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56802e3b-739f-4a8f-9c3d-78da53b9e20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x75a77b950cd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights saved in H5 format.\n",
    "model.load_weights('./results/multi_task_T5_tf_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0accd7a-392a-41d6-beff-12d2f6bb9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the saved directory.\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./results/multi_task_T5_tf/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dbeea26-119c-4f8e-9659-8b227dca10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {3: 'positive', 2: 'neutral', 1: 'negative', 0: 'irrelevant'}\n",
    "\n",
    "emotion_mapping = {\n",
    "    0: \"anger\", \n",
    "    1: \"fear\", \n",
    "    2: \"joy\", \n",
    "    3: \"love\", \n",
    "    4: \"sadness\", \n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "def decode_sentiment(label_code):\n",
    "    return sentiment_mapping.get(label_code, \"Unknown\")\n",
    "\n",
    "def decode_emotion(label_code):\n",
    "    return emotion_mapping.get(label_code, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8965e5d-1162-41bf-9ded-5b20622d8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, task):\n",
    "    if task in ['emotion', 'sentiment']:\n",
    "        # Tokenize the input with a moderate max_length.\n",
    "        inputs = tokenizer(text, return_tensors=\"tf\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "        logits = model(inputs, task=task, training=False)\n",
    "        # Get the predicted label index.\n",
    "        pred_idx = int(tf.argmax(logits, axis=1).numpy()[0])\n",
    "        # Decode the prediction using our mapping functions.\n",
    "        if task == \"sentiment\":\n",
    "            return decode_sentiment(pred_idx)\n",
    "        elif task == \"emotion\":\n",
    "            return decode_emotion(pred_idx)\n",
    "    \n",
    "    elif task == 'summary':\n",
    "        # Prepend the summarization prefix as required by T5.\n",
    "        input_text = \"summarize: \" + text\n",
    "        # Tokenize with a longer max_length for summarization inputs.\n",
    "        inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "        # Use the underlying T5 model's generate function.\n",
    "        generated_ids = model.t5.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        # Decode the generated token ids to text.\n",
    "        summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported task type. Use 'emotion', 'sentiment', or 'summary'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac20aae-b9c4-454b-96f9-4b2c4f4f39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Prediction: positive\n",
      "Emotion Prediction: joy\n",
      "Summary Output: This is an example text that we want to summarize.\n"
     ]
    }
   ],
   "source": [
    "# Test sentiment classification\n",
    "sentiment_result = predict(\"smart guy, i would surely hire\", task=\"sentiment\")\n",
    "print(\"Sentiment Prediction:\", sentiment_result)\n",
    "\n",
    "# Test emotion classification\n",
    "emotion_result = predict(\"smart guy, i would surely hire\", task=\"emotion\")\n",
    "print(\"Emotion Prediction:\", emotion_result)\n",
    "\n",
    "# Test summarization\n",
    "summary_result = predict(\"This is an example text that we want to summarize.\", task=\"summary\")\n",
    "print(\"Summary Output:\", summary_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336763bb-60cb-490f-803e-fb9803ad4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart guy, i would surely hire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487addc-95e9-451f-8fa7-7dd6c572ab0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
